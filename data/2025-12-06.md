<div id=toc></div>

# Table of Contents

- [eess.IV](#eess.IV) [Total: 2]


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [1] [Structure-Aware Adaptive Kernel MPPCA Denoising for Diffusion MRI](https://arxiv.org/abs/2512.04586)
*Ananya Singhal,Dattesh Dayanand Shanbhag,Sudhanya Chatterjee*

Main category: eess.IV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Diffusion-weighted MRI (DWI) at high b-values often suffers from low signal-to-noise ratio (SNR), making image quality poor. Marchenko-Pastur PCA (MPPCA) is a popular method to reduce noise, but it uses a fixed patch size across the whole image, which doesn't work well in regions with different structures. To address this, we propose an adaptive kernel MPPCA (ak-MPPCA) that selects the best patch size for each voxel based on its local neighborhood. This improves denoising performance by better handling structural variations.

</details>


### [2] [Multi Task Denoiser Training for Solving Linear Inverse Problems](https://arxiv.org/abs/2512.04709)
*Clément Bled,François Pitié*

Main category: eess.IV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Plug-and-Play Priors (PnP) and Regularisation by Denoising (RED) have established that image denoisers can effectively replace traditional regularisers in linear inverse problem solvers for tasks like super-resolution, demosaicing, and inpainting. It is now well established in the literature that a denoiser's residual links to the gradient of the image log prior (Miyasawa and Tweedie), enabling iterative, gradient ascent-based image generation (e.g., diffusion models), as well as new methods for solving inverse problems. Building on this, we propose enhancing Kadkhodaie and Simoncelli's gradient-based inverse solvers by fine-tuning the denoiser within the iterative solving process itself. Training the denoiser end-to-end across the solver framework and simultaneously across multiple tasks yields a single, versatile denoiser optimised for inverse problems. We demonstrate that even a simple baseline model fine-tuned this way achieves an average PSNR improvement of +1.34 dB across six diverse inverse problems while reducing the required iterations. Furthermore, we analyse the fine-tuned denoiser's properties, finding that its optimisation objective implicitly shifts from minimising standard denoising error (MMSE) towards approximating an ideal prior gradient specifically tailored for guiding inverse recovery.

</details>
